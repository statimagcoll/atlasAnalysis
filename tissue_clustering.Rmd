---
title: "Image Level Analysis"
author: "Simon Vandekar"
date: "2/24/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::knit_hooks$set(GPs=function(before, options, envir){
  if (before){
    cex=1.5
    par(mgp=c(1.7,.7,0), lwd=1.5, lend=2,
        cex.lab=0.8*cex, cex.axis=0.8*cex, cex.main=1*cex,
        mar=c(2.8,1.8,1.8,0), bty='l', oma=c(0,0,2,0))}
})
knitr::opts_chunk$set(echo = TRUE, fig.height = 4, fig.width = 4, GPs=TRUE, cache=TRUE, cache.lazy = FALSE)

library(raster)
library(parallel)
library(class)
library(fda)
library(lme4)
library(lmerTest)
library(emmeans)
library(sjPlot)
set.seed(1234)

```

## Functions

```{r functions}


# Downsamples the images
downsample = function(img, outimg, mask=NULL, fact=4, fun='mean'){
  # downsample
  if(is.character(img)){
    img = raster(img)
  }
  if(!is.null(mask)){
    if(is.character(mask)) mask = raster(mask)
    if(all(dim(img)==ceiling(dim(mask)/2))){
      mask = aggregate(mask, fact=2, fun=fun)
      fact = fact/2
      extent(img) = extent(mask)
    }
  }
  
  ans = aggregate(img, fact=fact, fun=fun, filename=outimg, overwrite=TRUE)
  ans = outimg
}




# Gaussian smoothing on the log scale. Divides by mean, applies transformation first, and then untransforms afterward.
# might not be the best approach. Can adjust arguments to do smoothing on the raw scale without a transformation.
gaussSmooth = function(img, outimg, mask, maskThr, sigma=5, transform=log10, invtransform=function(x) 10^x, offset=1/2, divmean = function(img){ cellStats(img, stat = 'mean')} ){
  message(basename(img))
  ans = raster(img)
  maskimg = raster(mask)
  values(ans)[values(maskimg)<=maskThr] = NA
  values(ans) = transform(values(ans)/divmean(ans) + offset)
  if(sigma>0){
    # set NA values outside mask
    #ans = focal(ans, focalWeight(ans, sigma, type='Gauss'), filename=outimg, overwrite=TRUE, fun=)
    # not a Gaussian smooth, but can handle boundaries better
    ans = focal(ans, focalWeight(ans, sigma, type='circle'), filename=outimg, overwrite=TRUE, fun='sum', na.rm=TRUE)
                #fun=function(x, na.rm=TRUE){ prod(x+1, na.rm=na.rm)^(1/sum(!is.na(x)))-1}
    # set NAs to zero
    values(ans) = invtransform(values(ans))-offset
    values(ans)[is.na(values(ans))] = 0
    writeRaster(ans, filename=outimg, overwrite=TRUE)
  }
  ans = outimg
}





# need to check that this does the same thing as Moran function when NAs are present
# reference: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0126158
moran = function (x, y=NULL, atlas, w = matrix(c(1, 1, 1, 1, 0, 1, 1, 1, 1), 3, 3)) 
{
    if(is.null(y)) y = x
    if(is.character(x)) x = raster(x)
    if(is.character(y)) y = raster(y)
    if(is.character(atlas)) atlas = raster(atlas)
    z <- x - cellStats(x, mean)
    z2 <- y - cellStats(y, mean)
    wZiZj <- focal(z2, w = w, fun = "sum", na.rm = TRUE, pad = TRUE)
    wZiZj <- wZiZj * z
    wZiZj <- c(by(values(wZiZj), values(atlas), sum))
    zsq <- sqrt(c(by(values(z^2), values(atlas), sum)))
    # ineffecient if x=y
    z2sq <- sqrt(c(by(values(z2^2), values(atlas), sum)))
    n <- table(values(atlas)) - c(by(values(is.na(z) | is.na(z2)), values(atlas), sum)) # ncell(z) - cellStats(is.na(z) | is.na(z2), "sum")
    values(z) = as.numeric(!is.na(values(z)) & !is.na(values(z2)))
    W <- c(by(values(focal(z, w = w, fun = "sum", na.rm = TRUE, pad = TRUE)), values(atlas), sum)) #cellStats(focal(z, w = w, fun = "sum", na.rm = TRUE, pad = TRUE), sum)
    NS0 <- n/W
    mI <- NS0 * wZiZj/zsq/z2sq
    return(mI)
}




# This function takes k-means centers and assigns each pixel to one of the centroids. Always divides the image by the mean adds offset and applies transformation.
# @param imgnames tif image names for markers used in the clustering
# @param centers kmeans centroid coordinates, with columns in the same order as imgnames
# @param weights Multiplicative weights applied in model fitting
# @param outname filename for output image
labelImage = function(imgnames, centers, weights, mask, maskThr=0, outname, transform = log10, offset=1/2){
  imgs = stack(imgnames)
  mask = raster(mask)
  values(imgs)[ values(mask)<=maskThr ] = NA
  #mins = apply(values(imgs), 2, function(v){min(v[v>0], na.rm=TRUE)})
  #values(imgs) = sweep(values(imgs), 2, mins, FUN="+")
  #values(imgs) = sweep(values(imgs), 2, colMeans(values(imgs), na.rm=TRUE), FUN='/') + offset
  imgout = calc(imgs, fun = function(vec){
    nas = apply(is.na(vec), 1, any)
    ans = rep(0, nrow(vec))
    vec = transform(vec[!nas,]+offset)
    if(nrow(vec)!=0){
      ans[!nas] = as.numeric(as.character(c(knn1(centers, test=sweep(vec, 2, weights, '*'), cl=1:nrow(centers)) ) ))
    }
    return(ans)
  }, filename=outname, overwrite=TRUE)
  return(outname)
}


# computes things for signal to noise ratio
snr = function(imgname, mask, maskThr=0, outname, transform = log10, offset=1/2){
  imgs = raster(imgname)
  mask = raster(mask)
  values(imgs)[ values(mask)<=maskThr ] = NA
  mu = cellStats(imgs, 'mean')
  sigma = cellStats(imgs, stat='sd')
  
  #values(imgs) = transform(values(imgs)/mu+offset)
  #snrTransform = cellStats(imgs, 'mean')/cellStats(imgs, 'sd')
  c(mu=mu, sigma=sigma)
}




# Function to get subset of data from the images
#' @param imgs character vector, matrix, or data frame of images to load. Rows are slides/regions/subjects; columns are images.
#' @param masks character vector of masks defining where there is tissue in the image
#' @param maskThr numeric noninclusive lower threshold for mask
#' @param subsamp integer specifying the factor to subsample each dimension of the image.
#' @return A data frame with the subsample of imaging data for all of the images. Images are identified by the rownames of the imgs variable.
subsampImgs = function(imgs, masks, maskThr=0, subsamp=4, transform=log10, offset=1/2 ){
  imgs = as.data.frame(imgs)
  ss = 1:nrow(imgs)
  imgnames = names(imgs)
  # samples every 4th downsampled voxel from the DAPI
  subsamp = rep(c(FALSE, TRUE), c(subsamp-1, 1))
  
  locData = do.call(rbind, lapply(ss, function(rind){
    message(rownames(imgs)[rind])
    # get DAPI for this slideRegion
    img = raster(masks[rind])
    
    arrIndsImg = img
    values(arrIndsImg) = outer(rep(subsamp, length.out=nrow(img) ), rep(subsamp, length.out=ncol(img) ), FUN = "&")
    # indsImg selects the pixels that will be used to build the model
    indsImg = (img>dapiThr & arrIndsImg)
    rm(arrIndsImg, img)
    
    # for channel
    names(imgnames) = imgnames
    res = as.data.frame(do.call(cbind, mclapply(imgnames, function(marker){
      # gets row for this subject
      img = raster(imgs[rind, marker])
      
      # select target voxels
      res = values(img)[ values(indsImg) ]
      res = transform(res + offset)
    }, mc.cores = 2 )))
    # set NAs
    res[ , imgnames[which(!imgnames %in% names(res)) ] ] = NA
    # reorder them to be the original
    res = res[ ,imgnames]
    # return result for this slideRegion
    res[,'ID'] = rownames(imgs)[rind]
    res
  }
  ) ) # combine them all into one data frame
}
```



## Overview

This first code chunk sets up the data frame `sr` that contains directories to all of the images.
It checks some dimensions of images, because the mask was not the right dimension for some images.
We need to get a mask from Joe that excludes regions where the tissue came off the slide across the rounds of staining.

```{r dataSetup, echo=FALSE}
#### CODE CHUNK FROM RUBY ####

# markers to use:
# These are modifiable. location markers are used for clustering, so you can expand this to use more markers for clustering
locationMarkers = c('PANCK', 'COLLAGEN', 'VIMENTIN', 'SOX9', 'OLFM4', 'NAKATPASE', 'MUC2')
immuneMarkers = c('CD3D', 'CD8', 'CD4', 'CD68')

#combine the csv files
slides <- c('MAP00083_0000_02_01','MAP00347_0000_03_02','MAP00377_0000_01_01',
            'MAP00392_0000_01F_01','MAP00411_0000_01A_0','MAP00546_0000_02_02',
            "MAP00696_0000_01_01","MAP00866_0000_02_02","MAP01391_0000_01_01",
            "MAP01938_0000_0E_01","MAP02112_0000_02_02","MAP02235_0000_03_04",
            "MAP02487_0000_01_01","MAP02951_0000_02_02","MAP03077_0000_01_01",
            "MAP03252_0000_06_04","MAP03331_0000_01_01","MAP03361_0000_01A_01",
            "MAP03410_0000_06A_02","MAP04255_0000_02_02","MAP04781_0000_01_01",
            "MAP04814_0000_01_03","MAP05212_0000_01_01","MAP05216_0000_01_01",
            "MAP05363_0000_06_04","MAP05994_0000_01A_0","MAP06025_0000_02_01",
            "MAP06134_0000_01_01","MAP06147_0000_03_03","MAP06310_0000_02_03")
threshold <- list()
for(i in 1:length(slides)){
  slide_i <- slides[i]
  file <- sprintf('/media/disk2/atlas_mxif/T_cell_quantification/Images/%s/%s_thresholds.csv',slide_i,slide_i)
  threshold_i <- read.csv(file)
  threshold[[i]] <- threshold_i
}
threshold <- do.call(rbind,threshold)
nulls <- which(threshold$Threshold==0)
threshold <- threshold[-nulls,]
threshold$slideRegion <- sprintf('%s_region%s',threshold$Slide,threshold$Region)
slideRegion <- unique(threshold$slideRegion)

size <- length(slides)
test_id <- sample(1:size,size/2,replace = F)
test_s <- slides[test_id]
test <- threshold[threshold$Slide%in%test_s,]
train_s <- slides[-test_id]
train <- threshold[threshold$Slide%in%train_s,]

idVars = c('Slide', 'Region', 'slideRegion')
sr = threshold[!duplicated(threshold$slideRegion), idVars]
immunePosMarkers = paste0(immuneMarkers, 'pos')
names(locationMarkers) = locationMarkers
allMarkers = c('DAPI', immuneMarkers, locationMarkers)
sr[, allMarkers ] = sapply(allMarkers, function(marker) file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/Adjusted Images', sr$Slide, sprintf('%s_ADJ_region_%03d.tif', marker, sr$Region) ) )
sr[,immunePosMarkers] = sapply(immuneMarkers, function(marker) file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/', sr$Slide, sprintf('%s_region_%03d_threshold.png', marker, sr$Region) ) )
sr[,'mask'] = file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/Adjusted Images', sr$Slide, sprintf('%s_%02d_TISSUE_MASK.tif', sr$Slide, sr$Region) )
sr[,'tumorMask'] = file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/BALANCED', sr$Slide, sprintf('Tumor_mask_region_%03d.png', sr$Region) )
sr[,'epiMask'] = file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/', sr$Slide, sprintf('%s_region_%03d_epi_mask.png', sr$Slide, sr$Region) )
sr[,'strMask'] = file.path('/media/disk2/atlas_mxif/T_cell_quantification/Images/', sr$Slide, sprintf('%s_region_%03d_stroma_mask.png', sr$Slide, sr$Region) )
# check that all the images exist
#sapply(sr[,c(immuneMarkers, locationMarkers) ], file.exists)


tts = read.csv('../data/TissueSubTypes Colon Map.csv')
# match capitalization
names(tts)[1] = 'TissueID'
sr$TissueID = sr$Slide
sr = merge(sr, tts, all.x=TRUE)
sr$adenSSLcrc = ifelse(sr$tissueSubType %in% c('Tub', 'TV'), 'adenoma', 
                        ifelse(sr$tissueSubType %in% c('HP', 'HP/SSL', 'SSL', 'Tub/SSL'), 'SSL', 
                               ifelse(sr$tissueSubType %in% c('CRC'), 'CRC', NA )) )

sr = sr[file.exists(sr$mask),]
sr = sr[ file.exists(sr$tumorMask),]

# MAP03361_0000_01A_01 has some bad regions due to tissue falling off the slide
#ss = grep('MAP00347_0000_03_02|MAP00866_0000_02_02|MAP04781_0000_01_01|MAP03410_0000_06A_02|MAP00083_0000_02_01|MAP00392_0000_01F_01|MAP00411_0000_01A_0|MAP02112_0000_02_02', sr$Slide)
ss = 1:nrow(sr)
# ss = 1:nrow(sr)

# checks that mask image matches dimensions of DAPI image (some didn't)
dimsMatch = apply(sr[ss,], 1, function(x) all(dim(raster(x['mask'])) == dim(raster(x['DAPI']))) )
ss = ss[dimsMatch]
```

### Markers

We will use the markers `r paste(locationMarkers, collapse=', ')`, as input to the Kmeans/NMF to define tissue classes and use the immune markers, `r paste(immuneMarkers, collapse=', ')`, to model infiltration into different compartments of the tissue or other spatial features within each tissue compartment.



## Downsample the images

Here, we downsample the images as input to Kmeans or NMF to define general tissue level clusters.
The downsampling returns the mean value of the pixels in the lower resolution image.
This substantially changes the distribution of the image intensities.
The `fun` argument of the downsample function can be used to change how smaller pixel values are combined into the larger value.
Median would be another reasonable choice, probably.
This takes a while to run.
It checks if the downsampled images exist for all the images first before running.
Change the `mc.cores` argument to use more cores.

The code chunk below also includes parameters for the kmeans and smoothing.

```{r downsample}
# factor to downsample, in each dimension (I think)
fact = 8

# origmarkers is going to contain the undownsampled images
origmarkers = sr

for(marker in c(allMarkers)){
  sr[,marker] = gsub('\\.tif$', '_downsampled.tif', origmarkers[,marker])
  cat(marker, '\n')
  if(!all(file.exists(sr[ss,marker]))){
    mcmapply(downsample, origmarkers[ss,marker], sr[ss,marker], MoreArgs=list(fact=fact), mc.cores = 12 )
  }
}

# This downsamples using the mode instead, which makes more sense for mask images.
# downsample(origmarkers$epiMask[1], sr$epiMask[1], mask=origmarkers[1, 'mask'], fact=fact, fun='modal')
# The mask argument is used, because some images were at a different resolution than the others, so I had to downsample some differently.
for(marker in c('epiMask', 'strMask', 'mask', 'tumorMask', immunePosMarkers)){
  sr[,marker] = gsub('\\.png$|\\.tif$', '_downsampled.tif', origmarkers[,marker])
  cat(marker, '\n')
  if(!all(file.exists(sr[ss,marker]))){
    message(marker)
    mcmapply(downsample, origmarkers[ss,marker], sr[ss,marker], mask=origmarkers[ss,'mask'], MoreArgs=list(fact=fact, fun='modal'), mc.cores = 40)
  }
}
# deletes all downsampled markers
# apply(sr[,allMarkers], 2, unlink)

# location label will be an image containing the tissue classes
sr$locationLabel = gsub('DAPI', 'location_label', sr[,'DAPI'])
# same, except the input data will be untransformed
sr$locationLabelUntransformed = gsub('DAPI', 'location_label_untransformed', sr[,'DAPI'])

# Threshold to define a mask from the DAPI images. Not sure why I chose this over the mask images.
dapiThr = 5
# factor for subsampling the images
fact = 8
# offset parameter for transformation
offset=0.001
# parameters for k-means
k = 4

# Create file names for smoothed downsampled images
slocationMarkers = paste(locationMarkers, 'smooth', sep="_")
nslocationMarkers = paste(locationMarkers, 'norm_smooth', sep="_")
#snormedMarkers = paste(normedMarkers, 'smooth', sep='_')
simmuneMarkers = paste(immuneMarkers, 'smooth', sep="_")
sr[,c(slocationMarkers, simmuneMarkers)] = apply(sr[,c(locationMarkers, immuneMarkers)], 2, function(x) gsub('\\.tif$', '_smoothed.tif', x) )
sr[,c(nslocationMarkers)] = apply(sr[,locationMarkers], 2, function(x) gsub('\\.tif$', 'normed_smoothed.tif', x) )
```



## Compute SNR
 
 Not run. Looking for useful quality metric for image data. No progress here, yet.
 
 
```{r, eval=FALSE}
for(marker in allMarkers[-1]){
  result = t(mcmapply(snr, sr[,marker], mask=sr[,'DAPI'], maskThr=5, mc.cores=4))
  colnames(result) = paste(marker, colnames(result), sep="_")
  sr[,colnames(result)] = result
}
```


## Image normalization

In this section I applied some image normalization while extracting data "subsampling" from the images. This subsample is selected to estimate the FDA normalization. Because we're not using it currently, this whole section is not run.

* I subsample at a factor of ``r fact``, so I sample 1/``r fact^2`` of the data on a grid.
* The DAPI image is thresholded at ``r dapiThr`` to define a mask. This doesn't work real well. I need a mask from Joe created comparing the first and last DAPI images.
* I used mean division with an offset of ``r offset`` and a `log10` transformation.
* I also used mean division without a `log10` transformation too.

Code to do the FDA is not run.

### Subsample the images first

This is actually not run now, because we don't need to extract the data if we aren't going to FDA normalization.

```{r getData, eval=FALSE}
# these are the markers we want to normalize
normalizationMarkers = locationMarkers

# no transformation
rownames(sr) = sr$slideRegion
normData = subsampImgs(sr[ss, normalizationMarkers], sr[ss, 'mask'], maskThr = dapiThr, subsamp = fact, offset = offset)
normData$ID = gsub('_region.*$', '', normData$ID)

# with transformation
unnormData = subsampImgs(sr[ss, normalizationMarkers], sr[ss, 'mask'], maskThr = dapiThr, subsamp = fact, offset = 0, transform = function(x) x)
unnormData$ID = gsub('_region.*$', '', normData$ID)


#layout(matrix(1:(ncol(normData)-1), nrow=2))
# To visualize the histograms
#apply(normData[,-6], 2, range)
#apply(normData[,-6], 2, hist)
#unlink(file.path(tempdir(), '*'), recursive = TRUE)
#converts a raster to a polygon object
#rasterToPolygons(r, fun=function(x){x>6})
```



This is the code chunk to apply FDA. I've removed these functions from this script, but if you want to use them we can reincorporate them.

```{r applyFDAnormalization, fig.width=16, fig.height=6, eval=FALSE}
normedMarkers = paste0(normalizationMarkers, '_norm')
sr[,normedMarkers] = apply(sr[, normalizationMarkers], 2, function(x) gsub('\\.tif', '_normed.tif', x) )

#debug(fdaEstimate)
#regFuncs = fdaEstimate(normData[,normalizationMarkers], normData$ID)
res2 = fdaNormalize(normData[,normalizationMarkers], dataSubjectID = normData$ID, imgSubjectID = sr$Slide[ss], imgs=sr[ss, normalizationMarkers], outimgs = sr[ss,normedMarkers], masks = sr[ss, 'mask'], offset=offset, nit = 1, nbasisNorm=2, norderNorm=2)
res4 = fdaNormalize(normData[,normalizationMarkers], dataSubjectID = normData$ID, imgSubjectID = sr$Slide[ss], imgs=sr[ss, normalizationMarkers], outimgs = sr[ss,normedMarkers], masks = sr[ss,'mask'], offset=offset, nit = 1, nbasisNorm=4, norderNorm=4)
res = res2

warps = fdaEstimate(normData[,normalizationMarkers], normData$ID, offset = 0.001)
par(ask = FALSE)
#plotreg.fd(warps[[1]])
#warps = fdaEstimate(normData[,normalizationMarkers], normData$ID, offset = 0, transform=function(x) x)
#plotreg.fd(warps[[4]])

# before and after normalization
#hist(10^(normData[ normData$ID=='MAP00866_0000_02_02_region1', 'SOX9']), breaks=60)
#hist(res$normedData[ normData$ID=='MAP00866_0000_02_02_region1', 'SOX9'], breaks=60)
#hist((res$normedData[ normData$ID=='MAP00866_0000_02_02', 'SOX9']), breaks=60)
#hist((res$normedData[ normData$ID=='MAP03361_0000_01A_01', 'SOX9']), breaks=60)

#by(normData[,'SOX9'], normData$ID, function(x) hist(x, breaks=60))
#by(normData[,'SOX9'], normData$ID, function(x) hist(x[x>log10(offset)], breaks=60))

# visualize the transformations
layout(mat=matrix(1:(length(unique(normData$ID)) *3), nrow=3, byrow=TRUE))
invisible(by(normData[,normalizationMarkers[1]], normData$ID, function(x) hist(x[x>log10(offset)], breaks = seq(log10(offset), log10(255+offset), length.out=60))))
invisible(by(res2$normedData[,normalizationMarkers[1]], normData$ID, function(x) hist(x[x>log10(offset)], breaks = seq(min(res2$normedData[,normalizationMarkers[1]]), max(res2$normedData[,normalizationMarkers[1]]), length.out=60)) ) )
invisible(by(res4$normedData[,normalizationMarkers[1]], normData$ID, function(x) hist(x[x>log10(offset)], breaks = seq(min(res4$normedData[,normalizationMarkers[1]]), max(res4$normedData[,normalizationMarkers[1]]), length.out=60)) ) )


```



## Spatial Smoothing

Here I use the gaussSmooth function to apply smoothing. It is not actually Gaussian smoothing, because the `raster` package did not have an efficient way to deal with edge effects using Gaussian smoothing, so it's actually homogeneous smoothing on a disk of radius sigma=10, which corresponds to 10 of the original pixel units (I think). This type of smoothing is not ideal.
It's applied to all of the location markers, which are being used to define broad tissue classes.

There might be code for smoothing.
I considered using the command line function `c3d` to do smoothing instead, which is likely more efficient and better (https://sourceforge.net/p/c3d/git/ci/master/tree/doc/c3d.md).
I will modify this to use that instead, when I get a chance.
Alternatively, we can wrap Python or another function for smoothing.

I smooth on the raw scale because it's less confusing than smoothing on the `log10` transformed scale. I'm not sure what's better.
Raw scale is more natural with what the downsampling has done already and less confusing in terms of keeping track of the transformations.

So the pipeline is

1. Downsample
2. Normalize (done here before smoothing)
3. Smooth

If this is the pipeline the code could be made more efficient.

```{r smoothing, eval=TRUE}
sigma=50

#gaussSmooth = function(img, outimg, mask, maskThr, sigma=5, transform=log10, invtransform=function(x) 10^x, offset=1/2, divmean = function(img){ cellStats(img, stat = 'mean')} )
# smooth on the raw
# still divides by the mean.
#marker=locationMarkers[1]; gaussSmooth()

### Code to look at smoothing kernel size
  #img = sr[2,marker]; outimg=sr[2,paste(marker, 'smooth', sep="_")]; mask=sr[2,'mask']; maskThr=dapiThr; sigma=sigma; transform=function(x) x; invtransform = function(x) x; offset=0
  # grayscale_colors <- gray.colors(100,            # number of different color levels 
  #                               start = 0.0,    # how black (0) to go
  #                               end = 1.0,      # how white (1) to go
  #                               gamma = 2.2,    # correction between how a digital 
  #                               # camera sees the world and how human eyes see it
  #                               alpha = NULL)   #Null=colors are not transparent
  # 
  # kernel = ans; values(kernel) = 0;
  # #values(ans)[is.na(values(ans))] = 0
  # smk = focalWeight(ans, 50, type='circle')
  # dim(smk)
  # inds = ceiling(dim(kernel)/2)[1:2]
  # kernel[ inds[1] + 1:nrow(smk), inds[2] + 1:nrow(smk)] = smk
  # plot(stack(ans, kernel), col=grayscale_colors)
  # st = brick(ans, kernel)
  # plotRGB(st, r = 1, g = 2)
  
for(marker in c(locationMarkers)){
  message(marker)
  if(!all(file.exists(sr[ss,paste(marker, 'smooth', sep="_")]))){
    #mcmapply(gaussSmooth, sr[ss,marker], sr[ss,paste(marker, 'smooth', sep="_")], sr[ss,'DAPI'], MoreArgs=list(maskThr=dapiThr, sigma=sigma, transform=function(x) x, invtransform = function(x) x, offset=0), mc.cores = 20 )
    mapply(gaussSmooth, sr[ss,marker], sr[ss,paste(marker, 'norm_smooth', sep="_")], sr[ss,'DAPI'], MoreArgs=list(maskThr=dapiThr, sigma=sigma, transform=function(x) x, invtransform = function(x) x, offset=0))
    mapply(gaussSmooth, sr[ss,marker], sr[ss,paste(marker, 'smooth', sep="_")], sr[ss,'DAPI'], MoreArgs=list(maskThr=dapiThr, sigma=sigma, transform=function(x) x, invtransform = function(x) x, offset=0, divmean=function(x) 1))
  }
}




# smoothing using c3d
# for(marker in c(locationMarkers, normedMarkers)){
#   message(marker)
#   sigma=20
#   invisible(mcmapply(gaussSmooth, sr[ss,marker], sr[ss,paste(marker, 'smooth', sep="_")], sr[ss,'mask'], MoreArgs=list(maskThr=dapiThr, sigma=sigma), mc.cores = 4 ))
# }
```







## Spatial Clusters


After smoothing, we then apply clustering. I was going to use NMF, but opted for Kmeans, for whatever reason. That density peak clustering is probably fast and a good option.


### Kmeans


* I subsample at a factor of ``r fact``, so I sample 1/``r fact^2`` of the data on a grid.
* The DAPI image is thresholded larger ``r dapiThr`` to define a tissue mask. This doesn't work real well. I need a mask from Joe created comparing the first and last DAPI images. I think the masks Eliot sent were at a different resolution and I didn't want to deal with it, so I created new masks.
* I used mean division with an offset of ``r offset`` and a `log10` transformation.
* I also used mean division without a `log10` transformation too.


This first chunk runs the k-means on the log transformed data.
It then uses that model to assign the labels to all of the pixels and writes them out as the `locationLabel` images.
```{r kmeans, eval=TRUE}
# extract the smooth un-normalized data from the images
kMeansMarkers = nslocationMarkers
kmeansData = subsampImgs(sr[ss, kMeansMarkers], sr[ss, 'mask'], maskThr = dapiThr, subsamp = fact)


set.seed(1234)
# fit the kmeans model
X = kmeansData[,kMeansMarkers]
X = apply(X, 2, function(x) ifelse(is.na(x), min(x, na.rm=TRUE), x))
sds = apply(X, 2, sd)
X = sweep(X, 2, sds, '/')
kmeansMod = kmeans(X, k, iter.max = 200, algorithm = 'MacQueen')

### ASSIGN LABELS
outimgs = sapply(ss, function(rind){ message(sr$slideRegion[rind]); labelImage(unlist(sr[rind,kMeansMarkers]), centers=kmeansMod$centers[,kMeansMarkers], weights=1/sds, mask=sr[rind, 'mask'], maskThr=dapiThr, outname=sr[rind,'locationLabel']) } )
```


This second chunk runs k-means on the untransformed data and then assigns the values to the `locationLabelUntransformed` images.
```{r kmeansUntransformed, eval=FALSE}
# extract the smooth un-normalized data from the images
kMeansMarkers = slocationMarkers
kmeansUntransformedData = subsampImgs(sr[ss, kMeansMarkers], sr[ss, 'mask'], maskThr = dapiThr, subsamp = fact, offset=0, transform = function(x) x)


set.seed(1234)
# fit the kmeans model
X = kmeansUntransformedData[,kMeansMarkers]
X = apply(X, 2, function(x) ifelse(is.na(x), min(x, na.rm=TRUE), x))
sds = apply(X, 2, sd)
X = sweep(X, 2, sds, '/')
kmeansMod = kmeans(X, k, iter.max = 200, algorithm = 'MacQueen')

### ASSIGN LABELS
outimgs = sapply(ss, function(rind){ message(sr$slideRegion[rind]); labelImage(unlist(sr[rind,kMeansMarkers]), centers=kmeansMod$centers[,kMeansMarkers], weights=1/sds, mask=sr[rind, 'mask'], maskThr=dapiThr, offset=0, transform=function(x) x, outname=sr[rind,'locationLabelUntransformed']) } )
```
